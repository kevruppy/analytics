TODO:

put sample data in cloud storage
make paths to sample data variables that will be exchanged depending on env (can be checked using print(platform.release().endswith("WSL2"))
)
--> eventually we need a mapping for that, maybe in csv, maybe in .py script itself
--> make sql scripts replayable by checking if data is already in table
this is the way:
create temp table tmp as select 1 as x

insert into tmp
select 1
where (select count(*) = 0 from tmp)

SELECT * from tmp


static website hosting in s3 for dbt docu?



for dbt cloud only postgres and bigquery possible!

cicd:
dbt_project_evaluator
https://github.com/dbt-checkpoint/dbt-checkpoint
jsonschema only for ci but not in VS Code...





- build dim & fact models
- check some BI tool
- check dbt cloud
- check github actions
- check github branching
- check how this project would work with BigQuery
- check best practives for dockerfile
- check if we set requirements the right way
- check scheduling
- cli script to build dbt project (or Python? maybe a class?)
--> https://docs.getdbt.com/reference/programmatic-invocations



out of scope:
- sqlfluff should lint dbt tests properly
- python formatters (no need for ruff, pylint is fine)
(add more data e.g. product info, web traffic, customer whatever, channel costs)

Links:
https://neon.tech/
https://blog.streamlit.io/host-your-streamlit-app-for-free/
pre-commit run --all-files
